{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(filename):\n",
    "    \"\"\"\n",
    "    grid_size - strictly positive\n",
    "    num_obstacles - non-negative\n",
    "    Next num_obstacles lines: <x>,<y> - strictly positive, denoting locations of obstacles\n",
    "    <x>,<y> - destination point\n",
    "    \"\"\"\n",
    "    lines = open(filename + \".txt\").read().splitlines()\n",
    "    grid_size = int(lines[0])\n",
    "    num_obstacles = int(lines[1])\n",
    "    obstacles = []\n",
    "    for i in range(num_obstacles):\n",
    "        obstacles.append(tuple(eval(lines[i+2])))\n",
    "    destination = tuple(eval(lines[num_obstacles+2]))\n",
    "    \n",
    "    return grid_size, obstacles, destination\n",
    "\n",
    "def write_output(mdp, policy):\n",
    "    translate_policy(policy)\n",
    "    output_file = open(\"output.txt\",\"w\")\n",
    "    for j in range(mdp.grid_size):\n",
    "        for i in range(mdp.grid_size):\n",
    "            output_file.write(policy[(i,j)][0])\n",
    "        output_file.write('\\n')\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self, grid_size, obstacles, destination, gamma):\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacles = obstacles\n",
    "        self.destination = destination\n",
    "        # Set up reward map for each state\n",
    "        self.reward = {}\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                self.reward[(i,j)] = -1\n",
    "        for o in obstacles:\n",
    "            self.reward[o] = -101\n",
    "        self.reward[destination] = 99\n",
    "        \n",
    "        # set up gamma\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def action(self, state, action):\n",
    "        \"\"\"\n",
    "        param: \n",
    "            state - (i,j) current state\n",
    "            action - 'up', 'left', 'down', 'right'\n",
    "        return:\n",
    "            next_state - new (i,j)\n",
    "        \"\"\"\n",
    "        if action == 'up':\n",
    "            if state[1] == 0:\n",
    "                return state\n",
    "            return (state[0], state[1]-1)\n",
    "        elif action == 'left':\n",
    "            if state[0] == 0:\n",
    "                return state\n",
    "            return (state[0]-1, state[1])\n",
    "        elif action == 'down':\n",
    "            if state[1] + 1 == self.grid_size:\n",
    "                return state\n",
    "            return (state[0], state[1]+1)\n",
    "        elif action == 'right':\n",
    "            if state[0] + 1 == self.grid_size:\n",
    "                return state\n",
    "            return (state[0]+1, state[1])\n",
    "        else:\n",
    "            print ('Invalid action: ' + action)\n",
    "            return\n",
    "        \n",
    "    def expect_utility(self, state, policy, utility):\n",
    "        actions = ['up', 'left', 'down', 'right']\n",
    "        expect_u = 0\n",
    "        for a in actions:\n",
    "            new_state = self.action(state, a)\n",
    "            if a == policy:\n",
    "                expect_u += 0.7 * utility[new_state]\n",
    "            else:\n",
    "                expect_u += 0.1 * utility[new_state]\n",
    "        return expect_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_interation(mdp, e):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        mdp - an MDP with states S, actions A(s), transition model P(s'|s,a), Reward R(s), discount gamma\n",
    "        e - epsilon tolerence\n",
    "    return:\n",
    "        utility\n",
    "    \"\"\"\n",
    "    # Set up utility map for each state\n",
    "    utility = {}\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            utility[(i,j)] = 0\n",
    "    new_utility = utility.copy()\n",
    "    \n",
    "    # initial delta - the maximum change in the utility of any state in an iteration\n",
    "    delta = 0\n",
    "    # value iteration\n",
    "    while (True):\n",
    "        utility = new_utility.copy()\n",
    "        delta = 0\n",
    "        for i in range(mdp.grid_size):\n",
    "            for j in range(mdp.grid_size):\n",
    "                s = (i,j)\n",
    "                if s == mdp.destination:\n",
    "                    new_utility[s] = mdp.reward[s]\n",
    "                else:\n",
    "                    new_utility[s] = mdp.reward[s] + mdp.gamma * max(mdp.expect_utility(s, 'up', utility), \n",
    "                                                                  mdp.expect_utility(s, 'left', utility),\n",
    "                                                                  mdp.expect_utility(s, 'right', utility),\n",
    "                                                                  mdp.expect_utility(s, 'down', utility))\n",
    "                if abs(new_utility[s] - utility[s]) > delta:\n",
    "                    delta = abs(new_utility[s] - utility[s])\n",
    "        \n",
    "        if (delta < e*(1-mdp.gamma)/mdp.gamma):\n",
    "            break\n",
    "    \n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_policy(mdp, utility):\n",
    "    policy = {}\n",
    "    # compute policies\n",
    "    for i in range(mdp.grid_size):\n",
    "        for j in range(mdp.grid_size):\n",
    "            state = (i,j)\n",
    "            policy[state] = ['up']\n",
    "            max_expect_utility = mdp.expect_utility(state, 'up', utility)\n",
    "            for p in ['down', 'right', 'left']:\n",
    "                if mdp.expect_utility(state, p, utility) > max_expect_utility:\n",
    "                    max_expect_utility = mdp.expect_utility(state, p, utility)\n",
    "                    policy[state] = [p]\n",
    "    # write obscatles and destination\n",
    "    for o in mdp.obstacles:\n",
    "        policy[o] = ['o']\n",
    "    policy[mdp.destination] = ['.']\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_policy(policy):\n",
    "    translate_map = {'up':['^'], 'left':['<'], 'down':['v'], 'right':['>']}\n",
    "    for key in policy:\n",
    "        if policy[key][0] in translate_map:\n",
    "            policy[key] = translate_map[policy[key][0]]\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013947248458862305\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "filename = 'dev_cases/input-0'\n",
    "grid_size, obstacles, destination = read_input(filename)\n",
    "gamma = 0.9\n",
    "e = 0.1\n",
    "\n",
    "mdp = MDP(grid_size, obstacles, destination, gamma)\n",
    "\n",
    "utility = value_interation(mdp, e)\n",
    "policy = find_policy(mdp, utility)\n",
    "\n",
    "write_output(mdp,policy)\n",
    "\n",
    "print (time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
